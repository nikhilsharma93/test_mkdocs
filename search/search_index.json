{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deployment Introduction This framework provides a way to set up model deployment from reusable and modular components. The aim is to generalize how models are deployed at DL, although the tools within this might be useful for other tasks as well. Tip You can add the model to a Google bucket and provide the model path as /festivus/<path_to_model_in_googlestorage> Concepts Component This is the most basic unit of work, that wraps a python function, and controls which variables are fed to it and which ones are returned. Every python function that needs to be run should be wrapped in a Component. See Component Function Requirements Pipeline An ordered collection of Components, that provides method to execute its components in either a serial or a multiprocessing fashion. A Component cannot be run on its own, and needs to go inside a pipeline. Function Library A set of commonly useful functions that can be imported from this library to build Components. See Contributing to Function Library I/O Handling Components, when put inside a pipeline, are not automatically connected. The user needs connect them by specifying, for each Component, where it is getting the input variables from, and what variables should be returned at the end of its execution. Variable Namespace Since a user controls all the variable flow in a pipeline, it is crucial to understand the scope of each variable created. Static Component Variables : These are visible only to the Component they are set with. Intended use case is to pass all those input variables to a Component function that have a static value and are not needed in any other Component. Consider this to be the equivalent of wrapping the function with functools.partial Runtime Component Variables : These can be configured to receive values at runtime (i.e the point where the corresponding component function will be run), and can be shared with any other component. Intended use case is to pass I/O values between different components as the pipeline runs. Static Pipeline Variables : These are visible to every Component that exists in the pipeline. Intended use case is to set all those input variables to a Component function that have a pre-determinable and static value, and are needed in two or more Components. As an alternative, you could use Static Component Variables and set them in each Component definition, but it could get redundant and tedious. Info Components handle automatic deletion of Runtime Component Variables (from the same pipeline) once no longer required. See Variable Deletion Routines Info Static Pipeline Variables can be accessed even by components that are not part of that pipeline. Initializing a Component This is the fist step to creating a Component. The user must provide a unique name for it, and a python function that it wraps. You can do it like Example # If the function exists in the function library, pass it as a string comp = Component ( name = 'my_comp' , func = 'func_in_library' ) # If it is a function defined elsewhere, pass the handle def my_func : ... comp = Component ( name = 'my_comp' , func = my_func ) You could pass additional arguments for debugging , and multiprocessing Initializing a Pipeline This is the fist step to creating a Pipeline. The user must provide a unique name for it. You can do it like Example pipeline = Pipeline ( name = 'my_pipeline' ) Building a Component Once initialized, you should configure I/O variable routines among other optional things. The user provides where a component gets its input variables from, and what output variables of a component function are recorded (to be used by other components, later in the pipeline). You can optionally do things like saving and printing the variable values at the end of a component execution. Setting Input Variables There are three ways of passing input values to a function of a component By setting Component level static_args for static values for that Component (the alternative being functools.partial ) By setting Component level runtime_args for values that are made available at runtime (eg. the output of a previous component in the pipeline). By setting Pipeline level static_args for static values for one/more Components in that Pipeline (the alternative being functools.partial for each Component that needs it). If you set a variable this way, you should only try to access it via Component level runtime_args , and not Component level static_args ( WARNING : This works only if a component is the first one in a pipeline) By passing a keyword argument for that value to the run or map method of a pipeline which this component is the first member of. The following cases explain ways to do all the above. Passing Static Component Variables Set static_args attribute of a Component. It accepts a dictionary, where each key is the name to set, and each value is the variable value to assign to the name. Example comp = Component ( name = '...' ) comp . static_args = dict ( products = [ 'usda:naip:rgbn:v1' ], bands = [ 'red' , 'green' , 'blue' ]) # Every time the Component's function is run, these values are passed to it. Passing Runtime Component Variables If a Component function should get some of its input values from a Runtime Component Variable or a Pipeline Variable (this could be either because the value is coming from a previous step in the pipeline at runtime, or because the value is coming from a preset Pipeline Variable), you should use the runtime_args attribute of a Component to let it know where it should get the inputs from. It accepts a dictionary (key, value) , where each key is the name of the variable (in the component's function definition) that the value is passed to, and each value is variable that would be passed. Every key is of str type, and every value takes either <component_obj>.future_variable.<variable_name> or <pipeline_obj>.future_variable.<variable_name> depending on where it is coming from. Note that you cannot provide any other variable as a value . Also, variable names starting with single or double underscore are prohibited. Example # Let's create two components that we intend to run serially def my_func1 ( tilesize ): return locals () comp1 = Component ( name = 'comp1' , func = my_func1 ) def my_func2 ( tilesize ): tilesize += 2 return locals () comp2 = Component ( name = 'comp2' , func = my_func2 ) # Now, if comp1 is the first in a pipeline, we need to pass an initial value for `tilesize` # One way to do this is to set it as a static arg comp1 . static_args = dict ( tilesize = 1024 ) # Alternatively, I could use functools.partial and not set a static_arg: my_func1 = partial ( my_func1 , tilesize = 1024 ) comp1 = Component ( name = 'comp1' , func = my_func1 ) # As for comp2, it gets the tilesize from whatever comp1 returns as tilesize. # To do this, specify the runtime_arg of comp2 to get its `tilesize` from the output value of `tilesize` of comp1 comp2 . runtime_args = dict ( tilesize = comp1 . future_variable . tilesize ) # NOTE: we should also ask comp1 to record its output `tilesize` so that comp2 can access it. # We look at it in the next section, \"Setting Output Variables\" Passing Static Pipeline Variables Use the add_pipeline_variable method of a Pipeline to set any Static Pipeline Variable. It accepts a dictionary, where each key is the name to set, and each value is the variable value to assign to the name. Access them via specifying the value in a Component's runtime_args attribute. Example pipeline = Pipeline ( name = '...' ) pipeline . add_pipeline_variable ( dict ( unpadded_tilesize = 1024 , pad = 0 )) # Now I can use this in any component. # For example, if I want to use this as the `tilesize` input argument for comp1 from the # previous example, I could do: comp1 . runtime_args = dict ( tilesize = pipeline . future_variable . unpadded_tilesize ) # And I will not need to do comp1.static_args = dict(tilesize=1024) # NOTE: It is illegal to pass in a pipeline variable as to static_args of a component comp1 . static_args = dict ( tilesize = pipeline . future_variable . unpadded_tilesize ) # Illegal Info See Variable Sharing Routines to understand what variable values might get overwritten Info For understanding the lifetime of all these variables, see Variable Deletion Routines Passing Pipeline-run Variables This is useful when a pipeline is wrapped inside another pipeline, and some arguments need to be passed during the execution of the outer pipeline to the inner pipeline. Use inner_pipeline.run(kwargs=<kwargs>) . See Wrapping Pipeline inside another Pipeline for an example. Passing Pipeline-map Variables Analogous to Passing Pipeline-run Variables , this method is useful when you are doing a map call on a pipeline instead of a run call (see Multiprocessing Across Pipeline ). You could do inner_pipeline.map(kwargs=<kwargs>) or inner_pipeline.map(n_times=<n_times>) . See Multiprocessing Examples Setting Output Variables A user can specify which variables from a function should the Component record at the end of its execution, so that it can be made available for use by other components in that/other pipeline(s). Use the to_record attribute of a Component to specify these variables. It accepts a dictionary (key, value) , where key is the name of the variable in the component's function and value is the name by which you want this variable to be stored by the component. Danger Names for value starting with single or double underscore are prohibited Example def my_func (): a1 = 5 a2 = 6 a1 += 7 return locals () comp = Component ( name = 'my_comp' , func = my_func ) comp . to_record = dict ( a1 = 'my_a1' , a2 = 'a2' ) # Whenever this comp is run as part of the pipeline, at the end of the execution of my_func, # the values `a1` and `a2` are stored as `my_a1` and `a2` in that component `comp`. # Any component that runs after this component can then access those values as # comp.future_variable.my_a1, and comp.future_variable.a2 # It is important to note that the value of a variable that gets recorded is the one that exists # just before the function returns. Intermediate values cannot be recorded/accessed. # In this example, the value of a1=5 cannot be recorded. Danger It is illegal to ask a component to record a variable but never use it. The framework will throw an exception. Compiling a Pipeline After you have built all the components, it's time to put them in a pipeline and run them. Example pipeline = Pipeline ( name = 'my_pipeline' ) # Let's say we want to add the two components comp1 and comp2 (for reference, defined under the section # \"Passing Runtime Component Variables\", but not important) # We add those components while compiling the pipeline pipeline . compile_pipeline ([ comp1 , comp2 ]) Info Optionally, you can pass additional arguments for parallelizing and logging when compiling a pipeline. See Multiprocessing Within Pipeline and Logging Running a Pipeline After you have compiled a pipeline, you can run it. You could either run it serially, or could multiprocess it. Example # To run it serially pipeline . run ( kwargs =< kwargs_to_pass > ) # where kwargs_to_pass is useful if you want to pass initial inputs to the *first* component in that pipeline. # If you want to multiprocess across pipelines (this is different from multiprocessing within a pipeline) pipeline . map ( kwargs =< kwargs_to_pass > , num_cores = num_cores ) # or pipeline . map ( n_times =< n_times > , num_cores = num_cores ) See an example for understand the run method. See Multiprocessing Across Pipeline for details on the map method. The output to stdout includes timestamp, PID stamp, component name stamp, and run count stamp (i.e i where that component is being executed for the i th time). These stamps are essentially helpful when multiprocessing to separate out messages and logs. Getting a Summary of Pipeline Runs The framework is able to give you a summary (number of runs, time taken, order of execution, etc.) for the Pipeline runs that happen. By default, it prints the summary to stdout, but you could also save a graph-like summary to an image file on disk. Example Pipeline . summarize () # This will print the summary to stdout Pipeline . summarize ( visualize_path = '<path>' ) # This will print and also save it to an image. Whenever the summarize method is called, it is able to provide the summary for all runs done until that point. As such, to get a summary for the whole deployment, add this as the last line of the code. Here is a summary of from the wind turbines example with multiprocessing Variable Sharing Routines Components allow variable sharing by providing a routine like <component_obj>.future_variable.<variable_name> . When recording a variable and passing it as a runtime_arg for another component via this routine, care must be taken if the component functions are doing in-place operations. Consider the following example Example inp_image = np . zeros (( 4 , 4 )) def my_func1 ( x ): print ( 'I am func1. I received: ' , x ) x += 5 return locals () def my_func2 ( x ): print ( 'I am func2. I received: ' , x ) x += 10 return locals () def my_func3 ( x ): print ( 'I am func3. I received: ' , x ) x += 2 return locals () comp1 = Component ( 'comp1' , func = my_func1 ) comp1 . static_args = dict ( x = inp_image ) comp1 . to_record = dict ( x = 'x' ) comp2 = Component ( 'comp2' , func = my_func2 ) comp2 . runtime_args = dict ( x = comp1 . future_variable . x ) comp3 = Component ( 'comp3' , func = my_func3 ) comp3 . runtime_args = dict ( x = comp1 . future_variable . x ) pipeline = Pipeline ( 'my_pipeline' ) pipeline . compile_pipeline ([ comp1 , comp2 , comp3 ]) pipeline . run () Output # Here, the output looks like this: I am func1 . I received : [[ 0. 0. 0. 0. ] [ 0. 0. 0. 0. ] [ 0. 0. 0. 0. ] [ 0. 0. 0. 0. ]] I am func2 . I received : [[ 5. 5. 5. 5. ] [ 5. 5. 5. 5. ] [ 5. 5. 5. 5. ] [ 5. 5. 5. 5. ]] I am func3 . I received : [[ 15. 15. 15. 15. ] [ 15. 15. 15. 15. ] [ 15. 15. 15. 15. ] [ 15. 15. 15. 15. ]] # Even though we were intending to pass the output of comp1 to comp3, the fact that comp2 did an # in-place modification to `x` in my_func2 meant that when it was time for `comp1.future_variable.x` # to get evaluated by comp3, it was already modified. The framework allows you to access component variables from components belonging to a different pipeline, but there are certain implications leading to them remaining in memory throughout the life of the program. See Variable Deletion Routines Variable Deletion Routines In consideration of being memory efficient, this framework runs a cleanup job after running every component, and deletes all (*) those variables that are no longer needed. Consider the following example Example comp1 = Component ( ... ) comp1 . to_record = dict ( c1_x = 'x' ) comp2 = Component ( ... ) comp2 . runtime_args = dict ( c2_x = comp1 . future_variable . x ) comp2 . to_record = dict ( x = 'x' ) comp3 = Component ( ... ) comp3 . runtime_args = dict ( c3_x = comp2 . future_variable . x ) pipeline = Pipeline ( ... ) pipeline . compile_pipeline ([ comp1 , comp2 , comp3 ]) When the pipeline is run, the order of execution of components is comp1->comp2->comp3 . After comp2 uses comp1.future_variable.x , it is no longer needed, because comp3 does not use it. As such, the framework goes ahead and deletes comp1.future_variable.x . This deletion behavior is True for all such variables with the following restrictions: Variables coming from Pipeline Static Args are not deleted. The lifetime of such variables are as long as that Pipeline object is in scope. Variables coming from a component that runs in another pipeline are not deleted. This is because the source pipeline does not know if the destination pipeline is running in a loop or not. In the above example, if comp1 was part of a different pipeline, then comp1.future_variable.x would not have been deleted. This aspect is crucial to understand when running pipelines in a loop and multiprocessing them (see Multiprocessing Within Pipeline ) Multiprocessing This framework provides easy ways to multiprocess pipeline runs. Ways to Multiprocess There are two ways one could multiprocess: within pipeline and across pipeline Multiprocessing Within Pipeline In this case the components of a pipeline run asynchronously, and if the pipeline is being run in a loop, each component in a given iteration of that loop is allowed to start processing as soon as the corresponding component from previous iteration completes its processing. As such, at any given instance, exactly one iteration-instance of a component will be running. Each component can define its max queue size which controls how far ahead a component can be in the loop iteration with respect to other components that receive runtime_args from it. Pipeline Execution Graph - Component vs Time To setup your pipeline to multiprocess this way, provide/do the following: parallelize=True when compiling the pipeline, like pipeline . compile_pipeline ([ comp1 , comp2 ], parallelize = True ) Call pipeline.wait() after calling the run method on it. Unless you call this, the async process will be launched but the program will not wait for their completion. If you are running the pipelines in a loop, you could call the wait method towards the end. See the Example on pipeline.wait() below. [ Optional; defaults to 1 ] queue_size on Component initialization, like comp = Component ( name =... , func =... , queue_size = 2 ) . This controls the max queue size mentioned above. [ Optional; defaults to None ] queue_timeout on Component initialization, like comp = Component ( name =... , func =... , queue_timeout = 100 ) . This controls the timeout (in seconds) on put and get methods. Should be >= 1 if provided. Else, if left to None, it blocks until it can get or put . See the Info on queue_timeout below. [ Optional; defaults to True ] run_async=False on Component initialization if you want that specific component to run in the main process and not in a fork. This is helpful for cases like tensorflow model prediction. If you have a component that runs a tf model prediction, then it is not a good idea to launch it in a fork: (i) Tensorflow handles its own multiprocessing and doesn't like to be run in a separate forked process, and (ii) even if you do so, you will have to make sure that no tf session was initialized in the main process. It's just better to run the model prediction in the main process (this will also guarantee that at any given instance, there is only one model prediction running - thereby using full resources). See the Info on run_async below for some more information. Example on pipeline.wait() for i in range ( 3 ): pipeline . run ( kwargs = dict ( key = i )) pipeline . wait () Info on queue_timeout Blocking on put method - If a component has executed its function and is trying to save the variable requested in its to_record but the queue is full (because other components are yet to use the recorded variables from previous iterations), it will be blocked until it can find a space in the queue. If a timeout is provided, and the blocking continues for more than those many seconds, then a ComponentTimeoutError is raised. If left to None, it will block infinitely (until there is space to put). Blocking on get method - If a component is trying to get an input runtime variable from the component it mentioned in its runtime_args but that component has not made the variable available yet (perhaps because it did not finish its execution yet), it will be blocked until it can get it. If a timeout is provided, and the blocking continues for more than those many seconds, then a ComponentTimeoutError is raised. If left to None, it will block infinitely (until it gets). Info on run_async If any components are set with run_async=False , then internally, the pipeline first launches all the async tasks, and then sequentially runs these non-async components in the main process. If running in a loop, then all async tasks (i.e from all the loop iterations) are asynchronously spun up, and then each the non-async components are run one after another. This does not mean that the non-async components are blocked until the async ones finish processing; they are only blocked until the async ones are spun up - which takes negligible amount of time. Multiprocessing when sharing variables across pipelines If a component of a pipeline is giving one or more of its output variables to a component in another pipeline, then you cannot set the first pipeline to parallelize in two cases: If the first pipeline is running in a loop. (if not, and) if the second pipeline is both, running in a loop and set to parallelize. The user is responsible for making sure they avoid these cases, as the framework is not designed to recognize such setup. If this is not followed, it might result in an infinite blocking after the first iteration of the source component, or an unexpected termination. See the Multiprocessing when sharing variables across pipelines example below. Serializable restriction If you multiprocess this way, you are restricted to have any variables passed to to_record be serializable. Note There is no harm in calling pipeline.wait() even when parallelize=False . As such, when writing a generic component function that loops over a pipeline, it is a good idea to include pipeline.wait() , to allow parallelizing without having to change the function definition. Multiprocessing when sharing variables across pipelines # This example shows what is (not) allowed when multiprocessing and sharing variables across pipelines. def main_func1 ( x ): return dict ( x = x ) def child_func1 ( x ): x += 10 main_comp1 = Component ( name = 'mc1' , func = main_func1 ) child_comp1 = Component ( name = 'cc1' , func = child_func1 ) main_comp1 . to_record = repeator ( 'x' ) main_comp1 . static_args = dict ( x = 5 ) child_comp1 . runtime_args = dict ( x = main_comp1 . future_variable . x ) main_p = Pipeline ( name = 'mp1' ) main_p . compile_pipeline ([ main_comp1 ], parallelize = True ) child_p = Pipeline ( name = 'cp1' ) child_p . compile_pipeline ([ child_comp1 ], parallelize = True ) main_p . run () main_p . wait () child_p . run () child_p . wait () # In the above example, I can parallelize `main_p` because `child_p` is not running # in a loop even though it is set to parallelize. # However, the following is invalid: main_p . run () main_p . wait () for i in range ( 3 ): child_p . run () child_p . wait () # Here, I cannot set `main_p` to parallelize because `child_p` is running in a loop while # being set to parallelize. # My options here are: # - Set `child_p` parallelize=False, and then I can parallelize `main_p`. # - Set `main_p` parallelize=False, and then I can parallelize `child_p`. # Another case that is invalid is to parallelize `main_p` and run it in a loop, irrespective of # how `child_p` is configured. for i in range ( 2 ): main_p . run () main_p . wait () child_p . run () child_p . wait () # This is invalid irrespective of whether `child_p` is set to parallelize and whether `child_p` # is running in a loop. Multiprocessing Across Pipeline In this case the components within a pipeline run sequentially, but the pipeline runs themselves can be parallelized. This is helpful if each component in your pipeline is not parallelizable (or scales across cores) by itself. To setup your pipeline to multiprocess this way, provide/do the following: Instead of calling pipeline.run , use the map method and provide the number of cores to multiprocess with: Example # If you have an kwargs to be passed to the pipeline (i.e to the first component in the pipeline), use the following signature pipeline . map ( kwargs = dict ( key = keys ), num_cores = num_cores ) # The value that goes to `kwargs` is a little different from the `run` method. See the INFO below # If you don't have any kwargs to pass, use the following signature pipeline . map ( n_times = n_times , num_cores = num_cores ) # where n_times is the number of times you want this pipeline to run. There is no need to call the wait method; it is handled internally. kwargs to map Passing kwargs to map is a little different than that in run method. Here, it expects a dictionary key: value , where each key is the name of an input argument of the function of the first component in the pipeline, and each value is a data structure (for eg. list, tuple) that supports value access by index, supports the len method to return its length, and it contains all the values that would be passed (one-at-a-time) to the function. Example # Let us say this is the function we need to run in parallel: def my_func ( a , b ): ... # We have values of a as 2, 3, 4 and b as 1, 6, 7. # Then do: inp_dict = dict ( a = [ 2 , 3 , 4 ], b = [ 1 , 6 , 7 ]) pipeline . map ( kwargs = inp_dict , num_cores = 3 ) Do not pass both kwargs and n_times You should pass exactly one of kwargs and n_times depending on your use case. If you pass a value in kwargs , then the number of times to run the pipeline is inferred from the length of the data structure of any of the values in the kwargs you pass. At this time, you cannot multiprocess within and across pipeline at the same time . As such, if you call the map method and have parallelize=True set during pipeline compilation, it will throw an exception. Also, when calling the map method, queue_size and run_async of a component are irrelevant. Comparison of Multiprocessing vs Not The following figure shows the advantage of multiprocessing within pipeline wrt the wind_turbines example The following figure shows the advantage of multiprocessing across pipeline wrt the sklearn example Debugging The framework tries to provide a bunch of options for debugging errors / unexpected outputs. These are especially helpful when multiprocessing. At the simplest, you could use the to_print or to_record attributes of a Component to debug when the output does not match the expected output. A bit further, you could create a debugger component and plug it in between the components you want to examine. With this debugger component, you can get all the data you want and anaylize / save variables. If you are trying to debug errors, then you could use the flag debug=True when initializing a component. In this mode the component prints every crucial tiny operation, and those prints formatted as mentioned previously to provide easier understanding of the execution stages. Formatted and well understood Exceptions may also be helpful in debugging. See Exceptions Contributing to Function Library Every function in the function library must exist in one of the following states: Component Function : These functions are meant to used with Components, and have some requirements. See Component Function Requirements Helper Function : These functions are meant to used within Component Functions ( NOT Components). They should just be regular python functions, returning whatever variables applicable. Example: model_predict_func Component Function Requirements These should be regular python functions with just one requirement: They should return either: ( Preferably ) Subset of locals() : Use this to provide the subset of variables that you think are all that an end-user will ever want to access. This will avoid returning unnecessary, intermediate, and garbage variables. The return statement would look something like return {'var1': value1, 'var2': value2, ...} ( Avoid as much as possible) locals() : The user can then choose whatever variables they want from this function as the outputs. The return statement would look like return locals() Note Do not return a subset of locals() just because you feel it is memory efficient. Whether you return locals() or not, you would just be returning a reference to it; there is no memory overhead. Repeator When providing input dictionaries for Component.to_record (as an example), it might get tedious to do something like: Component.to_record = dict(batch_imgs='batch_imgs', pad='pad', tile='tile', res='res') . Instead, you could use deploy.repeator to avoid the repetition: Example from appsci_utils.deployment.deploy import repeator Component . to_record = repeator ( 'batch_imgs' , 'pad' , 'tile' , 'res' ) # You could also pass kwargs Component . to_record = repeator ( 'batch_imgs' , 'pad' , tile = 'tile1' ) Exception Handling To make debugging easier, the framework handles some exceptions efficiently for you to quickly understand what went wrong. Erros are caught at different stages of the pipeline execution, but here is an overview of the type of exceptions the framework explicitly reports. ComponentInitializationError : When an error occurs at component initialization. ComponentRuntimeError : When an error occurs while the component is running. ComponentTimeoutError : (only applicable when multiprocessing within pipeline ) When a component timesout at get or put based on the timeout provided. See that section for more details. KeyExistingError : You would see this exception in case you try to overwrite an existing pipeline key by trying to add a Pipeline Variable (via .add_pipeline_variable ) that has a name clash. PipelineCompilationError : When an error occurs at pipeline compilation. PipelineInitializationError : When an error occurs at pipeline initialization. PipelineRuntimeError : When an error occurs while the pipeline is running, but usually before any of its components have started running (in which case it turns to a ComponentRuntimeError ). Any errors in the function definition, or otherwise are handled by python natively. Info When running in multiprocessing mode, if any error is caught by a child process, the parent prints it to stdout, shuts down every other child process, raises a SystemExit , and the program terminates. Quick Pointers You can pass keyword arguments to the kwarg kwargs of a pipeline.run command. These will internally be passed to (and only to) the first component in that pipeline. Never add anything to a pipeline after calling the compile_pipeline method on it If you are confused to see example functions return locals() or a similar dictionary, then read Contributing to Function Library and Component Function Requirements","title":"Concepts and Overview"},{"location":"#deployment","text":"","title":"Deployment"},{"location":"#introduction","text":"This framework provides a way to set up model deployment from reusable and modular components. The aim is to generalize how models are deployed at DL, although the tools within this might be useful for other tasks as well. Tip You can add the model to a Google bucket and provide the model path as /festivus/<path_to_model_in_googlestorage>","title":"Introduction"},{"location":"#concepts","text":"","title":"Concepts"},{"location":"#component","text":"This is the most basic unit of work, that wraps a python function, and controls which variables are fed to it and which ones are returned. Every python function that needs to be run should be wrapped in a Component. See Component Function Requirements","title":"Component"},{"location":"#pipeline","text":"An ordered collection of Components, that provides method to execute its components in either a serial or a multiprocessing fashion. A Component cannot be run on its own, and needs to go inside a pipeline.","title":"Pipeline"},{"location":"#function-library","text":"A set of commonly useful functions that can be imported from this library to build Components. See Contributing to Function Library","title":"Function Library"},{"location":"#io-handling","text":"Components, when put inside a pipeline, are not automatically connected. The user needs connect them by specifying, for each Component, where it is getting the input variables from, and what variables should be returned at the end of its execution.","title":"I/O Handling"},{"location":"#variable-namespace","text":"Since a user controls all the variable flow in a pipeline, it is crucial to understand the scope of each variable created. Static Component Variables : These are visible only to the Component they are set with. Intended use case is to pass all those input variables to a Component function that have a static value and are not needed in any other Component. Consider this to be the equivalent of wrapping the function with functools.partial Runtime Component Variables : These can be configured to receive values at runtime (i.e the point where the corresponding component function will be run), and can be shared with any other component. Intended use case is to pass I/O values between different components as the pipeline runs. Static Pipeline Variables : These are visible to every Component that exists in the pipeline. Intended use case is to set all those input variables to a Component function that have a pre-determinable and static value, and are needed in two or more Components. As an alternative, you could use Static Component Variables and set them in each Component definition, but it could get redundant and tedious. Info Components handle automatic deletion of Runtime Component Variables (from the same pipeline) once no longer required. See Variable Deletion Routines Info Static Pipeline Variables can be accessed even by components that are not part of that pipeline.","title":"Variable Namespace"},{"location":"#initializing-a-component","text":"This is the fist step to creating a Component. The user must provide a unique name for it, and a python function that it wraps. You can do it like Example # If the function exists in the function library, pass it as a string comp = Component ( name = 'my_comp' , func = 'func_in_library' ) # If it is a function defined elsewhere, pass the handle def my_func : ... comp = Component ( name = 'my_comp' , func = my_func ) You could pass additional arguments for debugging , and multiprocessing","title":"Initializing a Component"},{"location":"#initializing-a-pipeline","text":"This is the fist step to creating a Pipeline. The user must provide a unique name for it. You can do it like Example pipeline = Pipeline ( name = 'my_pipeline' )","title":"Initializing a Pipeline"},{"location":"#building-a-component","text":"Once initialized, you should configure I/O variable routines among other optional things. The user provides where a component gets its input variables from, and what output variables of a component function are recorded (to be used by other components, later in the pipeline). You can optionally do things like saving and printing the variable values at the end of a component execution.","title":"Building a Component"},{"location":"#setting-input-variables","text":"There are three ways of passing input values to a function of a component By setting Component level static_args for static values for that Component (the alternative being functools.partial ) By setting Component level runtime_args for values that are made available at runtime (eg. the output of a previous component in the pipeline). By setting Pipeline level static_args for static values for one/more Components in that Pipeline (the alternative being functools.partial for each Component that needs it). If you set a variable this way, you should only try to access it via Component level runtime_args , and not Component level static_args ( WARNING : This works only if a component is the first one in a pipeline) By passing a keyword argument for that value to the run or map method of a pipeline which this component is the first member of. The following cases explain ways to do all the above.","title":"Setting Input Variables"},{"location":"#passing-static-component-variables","text":"Set static_args attribute of a Component. It accepts a dictionary, where each key is the name to set, and each value is the variable value to assign to the name. Example comp = Component ( name = '...' ) comp . static_args = dict ( products = [ 'usda:naip:rgbn:v1' ], bands = [ 'red' , 'green' , 'blue' ]) # Every time the Component's function is run, these values are passed to it.","title":"Passing Static Component Variables"},{"location":"#passing-runtime-component-variables","text":"If a Component function should get some of its input values from a Runtime Component Variable or a Pipeline Variable (this could be either because the value is coming from a previous step in the pipeline at runtime, or because the value is coming from a preset Pipeline Variable), you should use the runtime_args attribute of a Component to let it know where it should get the inputs from. It accepts a dictionary (key, value) , where each key is the name of the variable (in the component's function definition) that the value is passed to, and each value is variable that would be passed. Every key is of str type, and every value takes either <component_obj>.future_variable.<variable_name> or <pipeline_obj>.future_variable.<variable_name> depending on where it is coming from. Note that you cannot provide any other variable as a value . Also, variable names starting with single or double underscore are prohibited. Example # Let's create two components that we intend to run serially def my_func1 ( tilesize ): return locals () comp1 = Component ( name = 'comp1' , func = my_func1 ) def my_func2 ( tilesize ): tilesize += 2 return locals () comp2 = Component ( name = 'comp2' , func = my_func2 ) # Now, if comp1 is the first in a pipeline, we need to pass an initial value for `tilesize` # One way to do this is to set it as a static arg comp1 . static_args = dict ( tilesize = 1024 ) # Alternatively, I could use functools.partial and not set a static_arg: my_func1 = partial ( my_func1 , tilesize = 1024 ) comp1 = Component ( name = 'comp1' , func = my_func1 ) # As for comp2, it gets the tilesize from whatever comp1 returns as tilesize. # To do this, specify the runtime_arg of comp2 to get its `tilesize` from the output value of `tilesize` of comp1 comp2 . runtime_args = dict ( tilesize = comp1 . future_variable . tilesize ) # NOTE: we should also ask comp1 to record its output `tilesize` so that comp2 can access it. # We look at it in the next section, \"Setting Output Variables\"","title":"Passing Runtime Component Variables"},{"location":"#passing-static-pipeline-variables","text":"Use the add_pipeline_variable method of a Pipeline to set any Static Pipeline Variable. It accepts a dictionary, where each key is the name to set, and each value is the variable value to assign to the name. Access them via specifying the value in a Component's runtime_args attribute. Example pipeline = Pipeline ( name = '...' ) pipeline . add_pipeline_variable ( dict ( unpadded_tilesize = 1024 , pad = 0 )) # Now I can use this in any component. # For example, if I want to use this as the `tilesize` input argument for comp1 from the # previous example, I could do: comp1 . runtime_args = dict ( tilesize = pipeline . future_variable . unpadded_tilesize ) # And I will not need to do comp1.static_args = dict(tilesize=1024) # NOTE: It is illegal to pass in a pipeline variable as to static_args of a component comp1 . static_args = dict ( tilesize = pipeline . future_variable . unpadded_tilesize ) # Illegal Info See Variable Sharing Routines to understand what variable values might get overwritten Info For understanding the lifetime of all these variables, see Variable Deletion Routines","title":"Passing Static Pipeline Variables"},{"location":"#passing-pipeline-run-variables","text":"This is useful when a pipeline is wrapped inside another pipeline, and some arguments need to be passed during the execution of the outer pipeline to the inner pipeline. Use inner_pipeline.run(kwargs=<kwargs>) . See Wrapping Pipeline inside another Pipeline for an example.","title":"Passing Pipeline-run Variables"},{"location":"#passing-pipeline-map-variables","text":"Analogous to Passing Pipeline-run Variables , this method is useful when you are doing a map call on a pipeline instead of a run call (see Multiprocessing Across Pipeline ). You could do inner_pipeline.map(kwargs=<kwargs>) or inner_pipeline.map(n_times=<n_times>) . See Multiprocessing Examples","title":"Passing Pipeline-map Variables"},{"location":"#setting-output-variables","text":"A user can specify which variables from a function should the Component record at the end of its execution, so that it can be made available for use by other components in that/other pipeline(s). Use the to_record attribute of a Component to specify these variables. It accepts a dictionary (key, value) , where key is the name of the variable in the component's function and value is the name by which you want this variable to be stored by the component. Danger Names for value starting with single or double underscore are prohibited Example def my_func (): a1 = 5 a2 = 6 a1 += 7 return locals () comp = Component ( name = 'my_comp' , func = my_func ) comp . to_record = dict ( a1 = 'my_a1' , a2 = 'a2' ) # Whenever this comp is run as part of the pipeline, at the end of the execution of my_func, # the values `a1` and `a2` are stored as `my_a1` and `a2` in that component `comp`. # Any component that runs after this component can then access those values as # comp.future_variable.my_a1, and comp.future_variable.a2 # It is important to note that the value of a variable that gets recorded is the one that exists # just before the function returns. Intermediate values cannot be recorded/accessed. # In this example, the value of a1=5 cannot be recorded. Danger It is illegal to ask a component to record a variable but never use it. The framework will throw an exception.","title":"Setting Output Variables"},{"location":"#compiling-a-pipeline","text":"After you have built all the components, it's time to put them in a pipeline and run them. Example pipeline = Pipeline ( name = 'my_pipeline' ) # Let's say we want to add the two components comp1 and comp2 (for reference, defined under the section # \"Passing Runtime Component Variables\", but not important) # We add those components while compiling the pipeline pipeline . compile_pipeline ([ comp1 , comp2 ]) Info Optionally, you can pass additional arguments for parallelizing and logging when compiling a pipeline. See Multiprocessing Within Pipeline and Logging","title":"Compiling a Pipeline"},{"location":"#running-a-pipeline","text":"After you have compiled a pipeline, you can run it. You could either run it serially, or could multiprocess it. Example # To run it serially pipeline . run ( kwargs =< kwargs_to_pass > ) # where kwargs_to_pass is useful if you want to pass initial inputs to the *first* component in that pipeline. # If you want to multiprocess across pipelines (this is different from multiprocessing within a pipeline) pipeline . map ( kwargs =< kwargs_to_pass > , num_cores = num_cores ) # or pipeline . map ( n_times =< n_times > , num_cores = num_cores ) See an example for understand the run method. See Multiprocessing Across Pipeline for details on the map method. The output to stdout includes timestamp, PID stamp, component name stamp, and run count stamp (i.e i where that component is being executed for the i th time). These stamps are essentially helpful when multiprocessing to separate out messages and logs.","title":"Running a Pipeline"},{"location":"#getting-a-summary-of-pipeline-runs","text":"The framework is able to give you a summary (number of runs, time taken, order of execution, etc.) for the Pipeline runs that happen. By default, it prints the summary to stdout, but you could also save a graph-like summary to an image file on disk. Example Pipeline . summarize () # This will print the summary to stdout Pipeline . summarize ( visualize_path = '<path>' ) # This will print and also save it to an image. Whenever the summarize method is called, it is able to provide the summary for all runs done until that point. As such, to get a summary for the whole deployment, add this as the last line of the code. Here is a summary of from the wind turbines example with multiprocessing","title":"Getting a Summary of Pipeline Runs"},{"location":"#variable-sharing-routines","text":"Components allow variable sharing by providing a routine like <component_obj>.future_variable.<variable_name> . When recording a variable and passing it as a runtime_arg for another component via this routine, care must be taken if the component functions are doing in-place operations. Consider the following example Example inp_image = np . zeros (( 4 , 4 )) def my_func1 ( x ): print ( 'I am func1. I received: ' , x ) x += 5 return locals () def my_func2 ( x ): print ( 'I am func2. I received: ' , x ) x += 10 return locals () def my_func3 ( x ): print ( 'I am func3. I received: ' , x ) x += 2 return locals () comp1 = Component ( 'comp1' , func = my_func1 ) comp1 . static_args = dict ( x = inp_image ) comp1 . to_record = dict ( x = 'x' ) comp2 = Component ( 'comp2' , func = my_func2 ) comp2 . runtime_args = dict ( x = comp1 . future_variable . x ) comp3 = Component ( 'comp3' , func = my_func3 ) comp3 . runtime_args = dict ( x = comp1 . future_variable . x ) pipeline = Pipeline ( 'my_pipeline' ) pipeline . compile_pipeline ([ comp1 , comp2 , comp3 ]) pipeline . run () Output # Here, the output looks like this: I am func1 . I received : [[ 0. 0. 0. 0. ] [ 0. 0. 0. 0. ] [ 0. 0. 0. 0. ] [ 0. 0. 0. 0. ]] I am func2 . I received : [[ 5. 5. 5. 5. ] [ 5. 5. 5. 5. ] [ 5. 5. 5. 5. ] [ 5. 5. 5. 5. ]] I am func3 . I received : [[ 15. 15. 15. 15. ] [ 15. 15. 15. 15. ] [ 15. 15. 15. 15. ] [ 15. 15. 15. 15. ]] # Even though we were intending to pass the output of comp1 to comp3, the fact that comp2 did an # in-place modification to `x` in my_func2 meant that when it was time for `comp1.future_variable.x` # to get evaluated by comp3, it was already modified. The framework allows you to access component variables from components belonging to a different pipeline, but there are certain implications leading to them remaining in memory throughout the life of the program. See Variable Deletion Routines","title":"Variable Sharing Routines"},{"location":"#variable-deletion-routines","text":"In consideration of being memory efficient, this framework runs a cleanup job after running every component, and deletes all (*) those variables that are no longer needed. Consider the following example Example comp1 = Component ( ... ) comp1 . to_record = dict ( c1_x = 'x' ) comp2 = Component ( ... ) comp2 . runtime_args = dict ( c2_x = comp1 . future_variable . x ) comp2 . to_record = dict ( x = 'x' ) comp3 = Component ( ... ) comp3 . runtime_args = dict ( c3_x = comp2 . future_variable . x ) pipeline = Pipeline ( ... ) pipeline . compile_pipeline ([ comp1 , comp2 , comp3 ]) When the pipeline is run, the order of execution of components is comp1->comp2->comp3 . After comp2 uses comp1.future_variable.x , it is no longer needed, because comp3 does not use it. As such, the framework goes ahead and deletes comp1.future_variable.x . This deletion behavior is True for all such variables with the following restrictions: Variables coming from Pipeline Static Args are not deleted. The lifetime of such variables are as long as that Pipeline object is in scope. Variables coming from a component that runs in another pipeline are not deleted. This is because the source pipeline does not know if the destination pipeline is running in a loop or not. In the above example, if comp1 was part of a different pipeline, then comp1.future_variable.x would not have been deleted. This aspect is crucial to understand when running pipelines in a loop and multiprocessing them (see Multiprocessing Within Pipeline )","title":"Variable Deletion Routines"},{"location":"#multiprocessing","text":"This framework provides easy ways to multiprocess pipeline runs.","title":"Multiprocessing"},{"location":"#ways-to-multiprocess","text":"There are two ways one could multiprocess: within pipeline and across pipeline","title":"Ways to Multiprocess"},{"location":"#multiprocessing-within-pipeline","text":"In this case the components of a pipeline run asynchronously, and if the pipeline is being run in a loop, each component in a given iteration of that loop is allowed to start processing as soon as the corresponding component from previous iteration completes its processing. As such, at any given instance, exactly one iteration-instance of a component will be running. Each component can define its max queue size which controls how far ahead a component can be in the loop iteration with respect to other components that receive runtime_args from it. Pipeline Execution Graph - Component vs Time To setup your pipeline to multiprocess this way, provide/do the following: parallelize=True when compiling the pipeline, like pipeline . compile_pipeline ([ comp1 , comp2 ], parallelize = True ) Call pipeline.wait() after calling the run method on it. Unless you call this, the async process will be launched but the program will not wait for their completion. If you are running the pipelines in a loop, you could call the wait method towards the end. See the Example on pipeline.wait() below. [ Optional; defaults to 1 ] queue_size on Component initialization, like comp = Component ( name =... , func =... , queue_size = 2 ) . This controls the max queue size mentioned above. [ Optional; defaults to None ] queue_timeout on Component initialization, like comp = Component ( name =... , func =... , queue_timeout = 100 ) . This controls the timeout (in seconds) on put and get methods. Should be >= 1 if provided. Else, if left to None, it blocks until it can get or put . See the Info on queue_timeout below. [ Optional; defaults to True ] run_async=False on Component initialization if you want that specific component to run in the main process and not in a fork. This is helpful for cases like tensorflow model prediction. If you have a component that runs a tf model prediction, then it is not a good idea to launch it in a fork: (i) Tensorflow handles its own multiprocessing and doesn't like to be run in a separate forked process, and (ii) even if you do so, you will have to make sure that no tf session was initialized in the main process. It's just better to run the model prediction in the main process (this will also guarantee that at any given instance, there is only one model prediction running - thereby using full resources). See the Info on run_async below for some more information. Example on pipeline.wait() for i in range ( 3 ): pipeline . run ( kwargs = dict ( key = i )) pipeline . wait () Info on queue_timeout Blocking on put method - If a component has executed its function and is trying to save the variable requested in its to_record but the queue is full (because other components are yet to use the recorded variables from previous iterations), it will be blocked until it can find a space in the queue. If a timeout is provided, and the blocking continues for more than those many seconds, then a ComponentTimeoutError is raised. If left to None, it will block infinitely (until there is space to put). Blocking on get method - If a component is trying to get an input runtime variable from the component it mentioned in its runtime_args but that component has not made the variable available yet (perhaps because it did not finish its execution yet), it will be blocked until it can get it. If a timeout is provided, and the blocking continues for more than those many seconds, then a ComponentTimeoutError is raised. If left to None, it will block infinitely (until it gets). Info on run_async If any components are set with run_async=False , then internally, the pipeline first launches all the async tasks, and then sequentially runs these non-async components in the main process. If running in a loop, then all async tasks (i.e from all the loop iterations) are asynchronously spun up, and then each the non-async components are run one after another. This does not mean that the non-async components are blocked until the async ones finish processing; they are only blocked until the async ones are spun up - which takes negligible amount of time. Multiprocessing when sharing variables across pipelines If a component of a pipeline is giving one or more of its output variables to a component in another pipeline, then you cannot set the first pipeline to parallelize in two cases: If the first pipeline is running in a loop. (if not, and) if the second pipeline is both, running in a loop and set to parallelize. The user is responsible for making sure they avoid these cases, as the framework is not designed to recognize such setup. If this is not followed, it might result in an infinite blocking after the first iteration of the source component, or an unexpected termination. See the Multiprocessing when sharing variables across pipelines example below. Serializable restriction If you multiprocess this way, you are restricted to have any variables passed to to_record be serializable. Note There is no harm in calling pipeline.wait() even when parallelize=False . As such, when writing a generic component function that loops over a pipeline, it is a good idea to include pipeline.wait() , to allow parallelizing without having to change the function definition. Multiprocessing when sharing variables across pipelines # This example shows what is (not) allowed when multiprocessing and sharing variables across pipelines. def main_func1 ( x ): return dict ( x = x ) def child_func1 ( x ): x += 10 main_comp1 = Component ( name = 'mc1' , func = main_func1 ) child_comp1 = Component ( name = 'cc1' , func = child_func1 ) main_comp1 . to_record = repeator ( 'x' ) main_comp1 . static_args = dict ( x = 5 ) child_comp1 . runtime_args = dict ( x = main_comp1 . future_variable . x ) main_p = Pipeline ( name = 'mp1' ) main_p . compile_pipeline ([ main_comp1 ], parallelize = True ) child_p = Pipeline ( name = 'cp1' ) child_p . compile_pipeline ([ child_comp1 ], parallelize = True ) main_p . run () main_p . wait () child_p . run () child_p . wait () # In the above example, I can parallelize `main_p` because `child_p` is not running # in a loop even though it is set to parallelize. # However, the following is invalid: main_p . run () main_p . wait () for i in range ( 3 ): child_p . run () child_p . wait () # Here, I cannot set `main_p` to parallelize because `child_p` is running in a loop while # being set to parallelize. # My options here are: # - Set `child_p` parallelize=False, and then I can parallelize `main_p`. # - Set `main_p` parallelize=False, and then I can parallelize `child_p`. # Another case that is invalid is to parallelize `main_p` and run it in a loop, irrespective of # how `child_p` is configured. for i in range ( 2 ): main_p . run () main_p . wait () child_p . run () child_p . wait () # This is invalid irrespective of whether `child_p` is set to parallelize and whether `child_p` # is running in a loop.","title":"Multiprocessing Within Pipeline"},{"location":"#multiprocessing-across-pipeline","text":"In this case the components within a pipeline run sequentially, but the pipeline runs themselves can be parallelized. This is helpful if each component in your pipeline is not parallelizable (or scales across cores) by itself. To setup your pipeline to multiprocess this way, provide/do the following: Instead of calling pipeline.run , use the map method and provide the number of cores to multiprocess with: Example # If you have an kwargs to be passed to the pipeline (i.e to the first component in the pipeline), use the following signature pipeline . map ( kwargs = dict ( key = keys ), num_cores = num_cores ) # The value that goes to `kwargs` is a little different from the `run` method. See the INFO below # If you don't have any kwargs to pass, use the following signature pipeline . map ( n_times = n_times , num_cores = num_cores ) # where n_times is the number of times you want this pipeline to run. There is no need to call the wait method; it is handled internally. kwargs to map Passing kwargs to map is a little different than that in run method. Here, it expects a dictionary key: value , where each key is the name of an input argument of the function of the first component in the pipeline, and each value is a data structure (for eg. list, tuple) that supports value access by index, supports the len method to return its length, and it contains all the values that would be passed (one-at-a-time) to the function. Example # Let us say this is the function we need to run in parallel: def my_func ( a , b ): ... # We have values of a as 2, 3, 4 and b as 1, 6, 7. # Then do: inp_dict = dict ( a = [ 2 , 3 , 4 ], b = [ 1 , 6 , 7 ]) pipeline . map ( kwargs = inp_dict , num_cores = 3 ) Do not pass both kwargs and n_times You should pass exactly one of kwargs and n_times depending on your use case. If you pass a value in kwargs , then the number of times to run the pipeline is inferred from the length of the data structure of any of the values in the kwargs you pass. At this time, you cannot multiprocess within and across pipeline at the same time . As such, if you call the map method and have parallelize=True set during pipeline compilation, it will throw an exception. Also, when calling the map method, queue_size and run_async of a component are irrelevant.","title":"Multiprocessing Across Pipeline"},{"location":"#comparison-of-multiprocessing-vs-not","text":"The following figure shows the advantage of multiprocessing within pipeline wrt the wind_turbines example The following figure shows the advantage of multiprocessing across pipeline wrt the sklearn example","title":"Comparison of Multiprocessing vs Not"},{"location":"#debugging","text":"The framework tries to provide a bunch of options for debugging errors / unexpected outputs. These are especially helpful when multiprocessing. At the simplest, you could use the to_print or to_record attributes of a Component to debug when the output does not match the expected output. A bit further, you could create a debugger component and plug it in between the components you want to examine. With this debugger component, you can get all the data you want and anaylize / save variables. If you are trying to debug errors, then you could use the flag debug=True when initializing a component. In this mode the component prints every crucial tiny operation, and those prints formatted as mentioned previously to provide easier understanding of the execution stages. Formatted and well understood Exceptions may also be helpful in debugging. See Exceptions","title":"Debugging"},{"location":"#contributing-to-function-library","text":"Every function in the function library must exist in one of the following states: Component Function : These functions are meant to used with Components, and have some requirements. See Component Function Requirements Helper Function : These functions are meant to used within Component Functions ( NOT Components). They should just be regular python functions, returning whatever variables applicable. Example: model_predict_func","title":"Contributing to Function Library"},{"location":"#component-function-requirements","text":"These should be regular python functions with just one requirement: They should return either: ( Preferably ) Subset of locals() : Use this to provide the subset of variables that you think are all that an end-user will ever want to access. This will avoid returning unnecessary, intermediate, and garbage variables. The return statement would look something like return {'var1': value1, 'var2': value2, ...} ( Avoid as much as possible) locals() : The user can then choose whatever variables they want from this function as the outputs. The return statement would look like return locals() Note Do not return a subset of locals() just because you feel it is memory efficient. Whether you return locals() or not, you would just be returning a reference to it; there is no memory overhead.","title":"Component Function Requirements"},{"location":"#repeator","text":"When providing input dictionaries for Component.to_record (as an example), it might get tedious to do something like: Component.to_record = dict(batch_imgs='batch_imgs', pad='pad', tile='tile', res='res') . Instead, you could use deploy.repeator to avoid the repetition: Example from appsci_utils.deployment.deploy import repeator Component . to_record = repeator ( 'batch_imgs' , 'pad' , 'tile' , 'res' ) # You could also pass kwargs Component . to_record = repeator ( 'batch_imgs' , 'pad' , tile = 'tile1' )","title":"Repeator"},{"location":"#exception-handling","text":"To make debugging easier, the framework handles some exceptions efficiently for you to quickly understand what went wrong. Erros are caught at different stages of the pipeline execution, but here is an overview of the type of exceptions the framework explicitly reports. ComponentInitializationError : When an error occurs at component initialization. ComponentRuntimeError : When an error occurs while the component is running. ComponentTimeoutError : (only applicable when multiprocessing within pipeline ) When a component timesout at get or put based on the timeout provided. See that section for more details. KeyExistingError : You would see this exception in case you try to overwrite an existing pipeline key by trying to add a Pipeline Variable (via .add_pipeline_variable ) that has a name clash. PipelineCompilationError : When an error occurs at pipeline compilation. PipelineInitializationError : When an error occurs at pipeline initialization. PipelineRuntimeError : When an error occurs while the pipeline is running, but usually before any of its components have started running (in which case it turns to a ComponentRuntimeError ). Any errors in the function definition, or otherwise are handled by python natively. Info When running in multiprocessing mode, if any error is caught by a child process, the parent prints it to stdout, shuts down every other child process, raises a SystemExit , and the program terminates.","title":"Exception Handling"},{"location":"#quick-pointers","text":"You can pass keyword arguments to the kwarg kwargs of a pipeline.run command. These will internally be passed to (and only to) the first component in that pipeline. Never add anything to a pipeline after calling the compile_pipeline method on it If you are confused to see example functions return locals() or a similar dictionary, then read Contributing to Function Library and Component Function Requirements","title":"Quick Pointers"},{"location":"DESIGN/","text":"The purpose of this document is to explain the framework design, the choices made, and places for improvement. Pipeline Design The Pipeline class is reponsible for controlling the runs of the components it is assigned. This includes things like maintaing what components are run serially, what are run in parallel, what components use the main process, what use forked processes, etc. It is also responsible for providing additional features like summarizing component runs . Assignment of components happens via the compile_pipeline method. The general lifecycle of a pipeline is as follows. Pipeline Initialization Each pipeline should have a unique name. A class level variable _mapper_pipeline_name_obj helps maintain references from these names to actual objects in memory. On initialization, it creates a number of variables, most of which are to track and record components execution status, run order, etc. It also initializes class level multiprocessing.Manager and multiprocessing.SyncManager objects. All the class level variables are meant to be initialized just once per fresh run of a code. Adding Pipeline Static Variables Each pipeline accepts unto itself a set of static variables that any component can access. Ideally, any component should not access static variables of a pipeline it does not belong to. This, however, is just a good practice case, and the framework as such does not impose restrictions on doing so. Thereby, the scope of variables added via this method is as long as that pipeline instance is in scope. Pipeline Compilation This is intended to be the last step before calling either a map or a run method on the pipeline. It's duty is to assign the pipeline with the specified set of components, their exact run order, and to make sure everything looks good (as far as it is possible to tell) in terms of the variables set. After doing its own checks, it asks each component (it has been assigned with) to do their own compilation (details of which are discussed in the section Compiling a Component ). Additionally, based on if the pipeline is set to parallelize, it initializes some Manager objects like shared dictionaries, lists, etc. Pipeline's run method This method is used to ask the pipeline to run either sequentially, or via multiprocessing within the pipeline . To run a pipeline means to run the components in the specified order (see what it means to run a component ). If it is the first time that the run method is being called on a pipeline instance, then it calls a _preset method, which inturn asks each component to preset (see what it means to preset a component ). If the pipeline is not set to parallelize, it just launches the _run method one-by-one for all the components it was compiled with (see what pipeline _run method does). When in parallelize mode, the pipeline provides each component with an option to be run in the main process instead of a forked process. The component is responsible for setting this flag. As such, in this parallelize mode, when a run is called on the pipeline, it loops through each component in the specified run order, and calls the _run method if it is to be run in a forked process, or just adds that component to a list of future jobs if it is to be run in the main process. The _run call in this case launches an asynchronous job under the hood, and returns immediately. As such, when the pipeline is done iterating over all the components, it now runs the future jobs (if any) sequentially in the main process, and then waits for any of the async jobs launched via _run . This is done by calling the wait method on the pipeline. Pipeline's map method This is an alternative to the run method, and is meant to be used when multiprocessing across pipelines . It is similar to multiprocessing.Pool.map method in that it takes the pipeline to run, and maps the given arguments across the specified number of cores. Under the hood, it just calls a new multiprocessing.Process on its run method (which inturn calls the _run method in sequential mode; the components within a pipeline as such are run sequentially, while the parallelism happens across the pipeline runs). Pipeline's _run method This is a private method that is meant to be called from the run method. It just takes in the component to run, and if it has to be run async, then it executes that component in a new multiprocessing.Process , else just executes it in the main process. Pipeline's summarize method This is intended to provide a summary of all the pipeline runs that have happened till the point this method gets called. It essentially just gets the run-times for each pipeline (and each of their components) and prints it in a formatted way to stdout. Optionally, it uses OpenCV to turn all these into a graph like diagram. Component Design The Component class is responsible for a single python function, starting from gathering its input arguments to running it to gathering and storing the required output values. Further, it is also responsible cleaning up its stored output variables ASAP. It is designed to receive execution calls [and report any exceptions] from [to] the pipeline it belongs to. Component Initialization Each component should have a unique name per pipeline. Components across different pipelines can have the same name. On initialization, every component should necessarily provide a name for it, and the python function to attach to it. Other arguments are optional. Most of them are only used when (multiprocessing within pipeline)(/test_mkdocs/multiprocessing-within-pipeline). The run_async flag controls whether or not the component should be run in the main process. Component Compilation This is called from the pipeline the component belongs to, and is responsible for initializing (if required) and/or updating existing variables. Right now, it does not anything when not parallelizing. Else, when parallelizing, it initializes shared objects, and converts any python objects to shared objects if they are meant to be accessed by other processes. See QueueDict (#queuedict) Preset a Component This does some checks on possible issues such as floating recorded outputs, missing runtime variables, etc. This only happens once per component. Running a Component This has several stages: Getting the arguments to run: This includes getting the static args, and fetching values for variables provided via runtime_args. When running in no-parallel (or parallelizing across pipelines) mode, each component just gets the variable from a name->variable dictionary. Whereas when parallelizing within pipelines, each component maintains a notion of an index (or the run count ) which notifies which iteration of the variable it is referring to. This is helpful when running the pipeline in a loop: A source component that executes really fast can do (let's say) 4 iterations in the time the receiver component does one iteration. In that case, whenever the receiver tries to get a variable from the source, it should tell it which iteration it refers to. Running the python function attached to it Storing all the required variables Printing and Saving variables if asked to (this is just a handy functionality that the framework provides with no overhead) Cleaning up: For each runtime_arg that a component uses, it notifies the source component that it has finished using this variable. The source is then responsible for deciding whether or not it should delete the variable from memory. See Component Variable Deletion Component Variable Deletion Each component is responsible for deciding whether or not to delete the variables they would have recorded after running their function. This check only happens when a component receives a delete call for a particular variable via a component that just finished using it (see Running a Component ). Essentially, the component checks for two things: Is there any consumer component that is not part of the same pipeline? If there is at least one, then the pipeline does not delete the variable, since it does not know if the receiver component is being run in a loop (in which case it might need the variable value for future iterations, which the source does not know of how many there will be). If there are none above, then it checks if all the receiver components have finished using that variable for that iteration. If yes, it goes ahead and clears that variable space. Component Exception Handling When running in no-parallel mode, everything is run in the main process, and any exception caught by the component is raised by its pipeline, and the program terminates as usual. However, when multiprocessing, the errors from children (which is where the component is actually run) need to be communicated to the parent. Due to the functionality of the pipeline, it cannot be hung on monitoring async components progress/exceptions: it has to execute the components that were set to run in the main process (via run_async=False ). As such, the current design is as follows: Whenever a component catches an exception, it reports it to its pipeline. The pipeline puts the execption message and traceback in a shared queue and signals a preset signal handler via os.kill . The signal handler prints the message along with the traceback, and kills the main process. Since all the child processes are started as daemons, they all get killed, and the program terminates. It is to note that due to this, the parent actually always raises a SystemExit whenever there is an exception in a child process. If there is a better way to handle things here, please let us know! QueueDict This is a custom data structure designed to meet the functionality required by the stored component variables when parallelizing within pipeline. In no-parallel mode, there is a dictionary that maps the variable name to its value, that any component can access. However, when parallelizing within pipeline, this dictionary needs to be shareable across processes. The multiprocessing.Manager.dict dictionary provides a shared dictionary, but does not have the concept of a maxsize like multiprocessing.Manager.Queue . The latter has that concept of maxsize but incorporates variable deletion in the get method. Our use case needs to separate the get and delete methods. Hence the need for this custom data structure. It provides a shareable dictionary that implements a put , get , and delete method, while taking a maxsize , such that when the dictionary reaches that size, it can block the put operations with an optional timeout . This class is made shareable across processes by registering it with multiprocessing.SyncManager . Due to some weird issues with python's custom exception handling in forked processes, it was not possible to raise exceptions from within any of the QueueDict's methods. As a workaround, any exception caught within those methods instantiates an _ExceptionRaiser object, which gets returned, and the caller then raises that exception if one is present.","title":"For Developers"},{"location":"DESIGN/#pipeline-design","text":"The Pipeline class is reponsible for controlling the runs of the components it is assigned. This includes things like maintaing what components are run serially, what are run in parallel, what components use the main process, what use forked processes, etc. It is also responsible for providing additional features like summarizing component runs . Assignment of components happens via the compile_pipeline method. The general lifecycle of a pipeline is as follows.","title":"Pipeline Design"},{"location":"DESIGN/#pipeline-initialization","text":"Each pipeline should have a unique name. A class level variable _mapper_pipeline_name_obj helps maintain references from these names to actual objects in memory. On initialization, it creates a number of variables, most of which are to track and record components execution status, run order, etc. It also initializes class level multiprocessing.Manager and multiprocessing.SyncManager objects. All the class level variables are meant to be initialized just once per fresh run of a code.","title":"Pipeline Initialization"},{"location":"DESIGN/#adding-pipeline-static-variables","text":"Each pipeline accepts unto itself a set of static variables that any component can access. Ideally, any component should not access static variables of a pipeline it does not belong to. This, however, is just a good practice case, and the framework as such does not impose restrictions on doing so. Thereby, the scope of variables added via this method is as long as that pipeline instance is in scope.","title":"Adding Pipeline Static Variables"},{"location":"DESIGN/#pipeline-compilation","text":"This is intended to be the last step before calling either a map or a run method on the pipeline. It's duty is to assign the pipeline with the specified set of components, their exact run order, and to make sure everything looks good (as far as it is possible to tell) in terms of the variables set. After doing its own checks, it asks each component (it has been assigned with) to do their own compilation (details of which are discussed in the section Compiling a Component ). Additionally, based on if the pipeline is set to parallelize, it initializes some Manager objects like shared dictionaries, lists, etc.","title":"Pipeline Compilation"},{"location":"DESIGN/#pipelines-run-method","text":"This method is used to ask the pipeline to run either sequentially, or via multiprocessing within the pipeline . To run a pipeline means to run the components in the specified order (see what it means to run a component ). If it is the first time that the run method is being called on a pipeline instance, then it calls a _preset method, which inturn asks each component to preset (see what it means to preset a component ). If the pipeline is not set to parallelize, it just launches the _run method one-by-one for all the components it was compiled with (see what pipeline _run method does). When in parallelize mode, the pipeline provides each component with an option to be run in the main process instead of a forked process. The component is responsible for setting this flag. As such, in this parallelize mode, when a run is called on the pipeline, it loops through each component in the specified run order, and calls the _run method if it is to be run in a forked process, or just adds that component to a list of future jobs if it is to be run in the main process. The _run call in this case launches an asynchronous job under the hood, and returns immediately. As such, when the pipeline is done iterating over all the components, it now runs the future jobs (if any) sequentially in the main process, and then waits for any of the async jobs launched via _run . This is done by calling the wait method on the pipeline.","title":"Pipeline's run method"},{"location":"DESIGN/#pipelines-map-method","text":"This is an alternative to the run method, and is meant to be used when multiprocessing across pipelines . It is similar to multiprocessing.Pool.map method in that it takes the pipeline to run, and maps the given arguments across the specified number of cores. Under the hood, it just calls a new multiprocessing.Process on its run method (which inturn calls the _run method in sequential mode; the components within a pipeline as such are run sequentially, while the parallelism happens across the pipeline runs).","title":"Pipeline's map method"},{"location":"DESIGN/#pipelines-_run-method","text":"This is a private method that is meant to be called from the run method. It just takes in the component to run, and if it has to be run async, then it executes that component in a new multiprocessing.Process , else just executes it in the main process.","title":"Pipeline's _run method"},{"location":"DESIGN/#pipelines-summarize-method","text":"This is intended to provide a summary of all the pipeline runs that have happened till the point this method gets called. It essentially just gets the run-times for each pipeline (and each of their components) and prints it in a formatted way to stdout. Optionally, it uses OpenCV to turn all these into a graph like diagram.","title":"Pipeline's summarize method"},{"location":"DESIGN/#component-design","text":"The Component class is responsible for a single python function, starting from gathering its input arguments to running it to gathering and storing the required output values. Further, it is also responsible cleaning up its stored output variables ASAP. It is designed to receive execution calls [and report any exceptions] from [to] the pipeline it belongs to.","title":"Component Design"},{"location":"DESIGN/#component-initialization","text":"Each component should have a unique name per pipeline. Components across different pipelines can have the same name. On initialization, every component should necessarily provide a name for it, and the python function to attach to it. Other arguments are optional. Most of them are only used when (multiprocessing within pipeline)(/test_mkdocs/multiprocessing-within-pipeline). The run_async flag controls whether or not the component should be run in the main process.","title":"Component Initialization"},{"location":"DESIGN/#component-compilation","text":"This is called from the pipeline the component belongs to, and is responsible for initializing (if required) and/or updating existing variables. Right now, it does not anything when not parallelizing. Else, when parallelizing, it initializes shared objects, and converts any python objects to shared objects if they are meant to be accessed by other processes. See QueueDict (#queuedict)","title":"Component Compilation"},{"location":"DESIGN/#preset-a-component","text":"This does some checks on possible issues such as floating recorded outputs, missing runtime variables, etc. This only happens once per component.","title":"Preset a Component"},{"location":"DESIGN/#running-a-component","text":"This has several stages: Getting the arguments to run: This includes getting the static args, and fetching values for variables provided via runtime_args. When running in no-parallel (or parallelizing across pipelines) mode, each component just gets the variable from a name->variable dictionary. Whereas when parallelizing within pipelines, each component maintains a notion of an index (or the run count ) which notifies which iteration of the variable it is referring to. This is helpful when running the pipeline in a loop: A source component that executes really fast can do (let's say) 4 iterations in the time the receiver component does one iteration. In that case, whenever the receiver tries to get a variable from the source, it should tell it which iteration it refers to. Running the python function attached to it Storing all the required variables Printing and Saving variables if asked to (this is just a handy functionality that the framework provides with no overhead) Cleaning up: For each runtime_arg that a component uses, it notifies the source component that it has finished using this variable. The source is then responsible for deciding whether or not it should delete the variable from memory. See Component Variable Deletion","title":"Running a Component"},{"location":"DESIGN/#component-variable-deletion","text":"Each component is responsible for deciding whether or not to delete the variables they would have recorded after running their function. This check only happens when a component receives a delete call for a particular variable via a component that just finished using it (see Running a Component ). Essentially, the component checks for two things: Is there any consumer component that is not part of the same pipeline? If there is at least one, then the pipeline does not delete the variable, since it does not know if the receiver component is being run in a loop (in which case it might need the variable value for future iterations, which the source does not know of how many there will be). If there are none above, then it checks if all the receiver components have finished using that variable for that iteration. If yes, it goes ahead and clears that variable space.","title":"Component Variable Deletion"},{"location":"DESIGN/#component-exception-handling","text":"When running in no-parallel mode, everything is run in the main process, and any exception caught by the component is raised by its pipeline, and the program terminates as usual. However, when multiprocessing, the errors from children (which is where the component is actually run) need to be communicated to the parent. Due to the functionality of the pipeline, it cannot be hung on monitoring async components progress/exceptions: it has to execute the components that were set to run in the main process (via run_async=False ). As such, the current design is as follows: Whenever a component catches an exception, it reports it to its pipeline. The pipeline puts the execption message and traceback in a shared queue and signals a preset signal handler via os.kill . The signal handler prints the message along with the traceback, and kills the main process. Since all the child processes are started as daemons, they all get killed, and the program terminates. It is to note that due to this, the parent actually always raises a SystemExit whenever there is an exception in a child process. If there is a better way to handle things here, please let us know!","title":"Component Exception Handling"},{"location":"DESIGN/#queuedict","text":"This is a custom data structure designed to meet the functionality required by the stored component variables when parallelizing within pipeline. In no-parallel mode, there is a dictionary that maps the variable name to its value, that any component can access. However, when parallelizing within pipeline, this dictionary needs to be shareable across processes. The multiprocessing.Manager.dict dictionary provides a shared dictionary, but does not have the concept of a maxsize like multiprocessing.Manager.Queue . The latter has that concept of maxsize but incorporates variable deletion in the get method. Our use case needs to separate the get and delete methods. Hence the need for this custom data structure. It provides a shareable dictionary that implements a put , get , and delete method, while taking a maxsize , such that when the dictionary reaches that size, it can block the put operations with an optional timeout . This class is made shareable across processes by registering it with multiprocessing.SyncManager . Due to some weird issues with python's custom exception handling in forked processes, it was not possible to raise exceptions from within any of the QueueDict's methods. As a workaround, any exception caught within those methods instantiates an _ExceptionRaiser object, which gets returned, and the caller then raises that exception if one is present.","title":"QueueDict"},{"location":"EXAMPLES/","text":"Basic Examples (For a full set of examples look under examples ) Basic Pipeline Code Let us say you want to build a two-component pipeline. Example import numpy as np from appsci_utils.deployment.deploy import Component , Pipeline , repeator inp_image = np . random . uniform ( 0 , 1 , size = ( 256 , 256 )) # Let's say the two functions I want to use are demo1 and demo2 from the function library # Create the first Component comp1 = Component ( name = 'demo1' , func = 'demo1' ) # From the definition of demo1, we need to pass inp_image, add_value (optional), and mul_value (optional) # We can pass these as Static Component variables since they are not needed elsewhere. comp1 . static_args = dict ( inp_image = inp_image , add_value = 10 , mul_value = 100 ) # We want to get the final value of `inp_image` since that will need to be passed to demo2. So, record it. comp1 . to_record = dict ( inp_image = 'inp_image' ) # Create the second Component comp2 = Component ( name = 'demo2' , func = 'demo2' ) # From the definition of demo1, we need to pass inp_image and threshold_value. The former is a # runtime argument that comes from the output of demo1, and threshold_value is a static argument. comp2 . static_args = dict ( threshold_value = 0.5 ) comp2 . runtime_args = dict ( inp_image = comp1 . future_variable . inp_image ) # i.e Get the value, at that time in the future when this function is executed, # stored by name `inp_image` in comp1 and pass it to comp2's function (i.e demo2) # as the kwarg value for `inp_image` # We do not want to record anything for this component since we don't need any output from it. # Create a pipeline and add these components pipeline = Pipeline ( name = 'demo' ) pipeline . compile_pipeline ([ comp1 , comp2 ]) # Run pipeline . run () Output RUNNING PIPELINE \"demo\" ... [ 2019 - 04 - 11 15 : 54 : 02 . 948031 ][ PID 21641 ][ n_time = 0 ] [ 2019 - 04 - 11 15 : 54 : 02 . 948138 ][ PID 21641 ][ Comp demo1 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 54 : 02 . 948152 ][ PID 21641 ][ Comp demo1 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 54 : 02 . 948393 ][ PID 21641 ][ Comp demo1 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 000151 [ 2019 - 04 - 11 15 : 54 : 02 . 948446 ][ PID 21641 ][ Comp demo2 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 54 : 02 . 948457 ][ PID 21641 ][ Comp demo2 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 54 : 02 . 948768 ][ PID 21641 ][ Comp demo2 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 000212 Printing Variables Code Example comp2 . to_print = [ 'dummy_variable' ] Output RUNNING PIPELINE \"demo\" ... [ 2019 - 04 - 11 15 : 56 : 42 . 365138 ][ PID 21956 ][ n_time = 0 ] [ 2019 - 04 - 11 15 : 56 : 42 . 365232 ][ PID 21956 ][ Comp demo1 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 56 : 42 . 365248 ][ PID 21956 ][ Comp demo1 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 56 : 42 . 365508 ][ PID 21956 ][ Comp demo1 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 000146 [ 2019 - 04 - 11 15 : 56 : 42 . 365544 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 56 : 42 . 365570 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 56 : 42 . 365774 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] to_print ... [ 2019 - 04 - 11 15 : 56 : 42 . 365792 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] [ to_print ] Value of dummy_variable [ 2019 - 04 - 11 15 : 56 : 42 . 365825 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] [ to_print ] 5 [ 2019 - 04 - 11 15 : 56 : 42 . 365960 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 000316 # Note that you do not need to add a variable to `to_record` to print it. Saving Variables Code Just like printing variables, you can also save them. To achieve this, use the to_save attribute of a Component. It accepts a dictionary, where a (key, value) can be of two types: - (key, value) = (str, str) : In this case, provide the key with the variable you are trying to save, and provide the full save path as the value . The method to save is inferred from the extension of the save path. Currently, saving numpy files ( .npy ), and images ( .png or .jpg ) is supported. - (key, value) = (str, function) : In this case, key remains the same as above, but in value provide a python function handle that would take in the positional argument (0th position) of the value of key at runtime and would save it according to the custom method. Example def dummy_save ( var ): print ( 'I am a dummy save method; will not save anything.' + 'But I confirm to have received the variable {}' . format ( var )) comp2 . to_save = dict ( inp_image = 'test_inp_image.npy' , dummy_variable = dummy_save ) Output RUNNING PIPELINE \"demo\" ... [ 2019 - 04 - 11 15 : 58 : 04 . 105548 ][ PID 22064 ][ n_time = 0 ] [ 2019 - 04 - 11 15 : 58 : 04 . 105655 ][ PID 22064 ][ Comp demo1 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 58 : 04 . 105669 ][ PID 22064 ][ Comp demo1 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 58 : 04 . 105897 ][ PID 22064 ][ Comp demo1 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 000140 [ 2019 - 04 - 11 15 : 58 : 04 . 105961 ][ PID 22064 ][ Comp demo2 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 58 : 04 . 105972 ][ PID 22064 ][ Comp demo2 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 58 : 04 . 106237 ][ PID 22064 ][ Comp demo2 ][ n_time = 0 ] Saving ... I am a dummy save method ; will not save anything . But I confirm to have received the variable 5 [ 2019 - 04 - 11 15 : 58 : 04 . 109076 ][ PID 22064 ][ Comp demo2 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 003014 # Also, test_inp_image.npy gets saved # Note that you do not need to add a variable to `to_record` to save it. Adding Static Pipeline Variables Code In our basic example, let us say we use demo3 from the function library instead of demo2 (and as such, comp3 instead of comp2 ), and we want to use the same value for the argument add_value in both demo1 and demo3 . One way to do this is to add it as static_args in both the components. The other way is to add it as a static pipeline variable just once, and let both the components access it. Example inp_image = np . random . uniform ( 0 , 1 , size = ( 256 , 256 )) # Initialize a pipeline, and set the pipeline variable `add_value` pipeline = Pipeline ( name = 'demo' ) pipeline . add_pipeline_variable ( dict ( add_value = 5 )) comp1 = Component ( name = 'demo1' , func = 'demo1' ) comp1 . static_args = dict ( inp_image = inp_image , mul_value = 100 ) comp1 . runtime_args = dict ( add_value = pipeline . future_variable . add_value ) comp1 . to_record = dict ( inp_image = 'inp_image' ) comp3 = Component ( name = 'demo3' , func = 'demo3' ) comp3 . runtime_args = dict ( inp_image = comp1 . future_variable . inp_image , add_value = pipeline . future_variable . add_value ) # Note that `compile_pipeline` should be the last operation performed on the pipeline before # calling the `.run` on it pipeline . compile_pipeline ([ comp1 , comp3 ]) pipeline . run () Output # Outputs RUNNING PIPELINE \"demo\" ... [ 2019 - 04 - 11 16 : 06 : 19 . 808513 ][ PID 22861 ][ n_time = 0 ] [ 2019 - 04 - 11 16 : 06 : 19 . 808601 ][ PID 22861 ][ Comp demo1 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 16 : 06 : 19 . 808614 ][ PID 22861 ][ Comp demo1 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 16 : 06 : 19 . 808841 ][ PID 22861 ][ Comp demo1 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 00012 8 [ 2019 - 04 - 11 16 : 06 : 19 . 808874 ][ PID 22861 ][ Comp demo3 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 16 : 06 : 19 . 808885 ][ PID 22861 ][ Comp demo3 ][ n_time = 0 ] Running Function ... I received the add_value as : 5 [ 2019 - 04 - 11 16 : 06 : 19 . 809067 ][ PID 22861 ][ Comp demo3 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 0000 94 Advanced Examples Wrapping Pipeline inside another Pipeline Consider the case when you have a pipeline that you want to run inside a loop. A simple way to do this would be like Example for x in ... : pipeline . run ( kwargs = dict ( inp_x = x )) But if the for loop had to be part of a Component, which in turn was a part of a Pipeline, then there is a little more to do. Consider the example where you have a list of two dlkeys, and want to run a model over each of them. The inner most pipeline then looks like (get image) + (run model prediction). Example def get_image ( key , products ): dltile = dl . raster . dltile ( key ) img , _ = dl . raster . ndarray ( ... ) return locals () def model_predict ( model_path , inp_image ): model = load_model ( model_path ) output = model . predict ( inp_image ) return locals () comp0 = Component ( name = 'get_image' , func = get_image ) comp0 . static_args = dict ( products = [ 'usda:naip:rgbn:v1' ]) comp0 . to_record = dict ( img = 'out_image' ) # We do not provide any value for `key` right now since it will come from an outer pipeline at runtime. comp1 = Component ( name = 'model_predict' , func = model_predict ) comp1 . static_args = dict ( model_path = '...' ) comp1 . runtime_args = dict ( inp_image = comp0 . future_variable . out_image ) inner_pipeline = Pipeline ( name = 'inner_pipeline' ) inner_pipeline . compile_pipeline ([ comp0 , comp1 ]) # Now we need an outer pipeline that has one component that gets the keys and another component # that loops over the keys and passes them sequentially to the inner pipeline. # If required, you could append other components to the outer pipeline, and they will get executed once the # inner pipeline is done processing all the keys in the loop. def get_keys (): keys = [ '1024:0:1.0:-3:2' , '1024:0:1.0:-10:4' ] return locals () def loop_keys ( keys , pipeline ): for i in loop_keys : pipeline . run ( kwargs = dict ( key = i )) # This is where the argument `key` is getting passed to `get_image` of comp0. # Note that declaration of such a function loop_keys limits what pipeline can be used with it: # The first component of that pipeline \"should\" accept an argument for `key` return locals () comp_main0 = Component ( name = 'get_keys' , func = get_keys ) comp_main0 . to_record = dict ( keys = 'dlkeys' ) comp_main1 = Component ( name = 'loop_keys' , func = loop_keys ) comp_main1 . static_args = dict ( pipeline = inner_pipeline ) comp_main1 . runtime_args = dict ( keys = comp_main0 . future_variable . dlkeys ) outer_pipeline = Pipeline ( name = 'outer_pipeline' ) outer_pipeline . compile_pipeline ([ comp_main0 , comp_main1 ]) outer_pipeline . run () Sharing Component Variables across Pipelines In the above example , you would notice that the same model is being loaded twice inside the inner pipeline. A better way to do this is to load the model once, in the outer pipeline, and then share the variable with the corresponding component in the inner pipeline. This applies to any such use case, where sharing removes redundancy. Example # Create a component for loading the model def model_loader ( model_path ): model = load_model ( model_path ) return locals () # Modify the definition of `model_predict` in the previous example to not load the model def model_predict ( model , inp_image ): output = model . predict ( inp_image ) return locals () # Create the outer pipeline comp_main0 = ... # Same as the get_keys component above comp_main1 = Component ( name = 'model_loader' , func = model_loader ) comp_main1 . static_args = dict ( model_path =... ) comp_main1 . to_record = dict ( model = 'loaded_model' ) comp_main2 = ... # Same as the loop_keys component above (called as comp_main1 in that example) outer_pipeline . compile_pipeline ([ comp_main0 , comp_main1 , comp_main2 ]) # Create the inner pipeline comp0 = ... # Same as in the above example comp1 = Component ( name = 'model_predict' , func = model_predict ) comp1 . runtime_args = dict ( inp_image = comp0 . future_variable . out_image , model = comp_main1 . future_variable . shared_model ) inner_pipeline . compile_pipeline ([ comp0 , comp1 ]) Task Examples Some caution needs to be taken when defining pipelines that work with DLTasks. The main tricky point is that Tasks can only take a serializable function as its input. Therefore, any pipeline that you want to run within Tasks should be wrapped in a python function. Example: Wind Turbines Example Multiprocessing Examples See an example for multiprocessing within pipeline See an example for multiprocessing across pipeline","title":"Examples"},{"location":"EXAMPLES/#basic-examples","text":"(For a full set of examples look under examples )","title":"Basic Examples"},{"location":"EXAMPLES/#basic-pipeline","text":"Code Let us say you want to build a two-component pipeline. Example import numpy as np from appsci_utils.deployment.deploy import Component , Pipeline , repeator inp_image = np . random . uniform ( 0 , 1 , size = ( 256 , 256 )) # Let's say the two functions I want to use are demo1 and demo2 from the function library # Create the first Component comp1 = Component ( name = 'demo1' , func = 'demo1' ) # From the definition of demo1, we need to pass inp_image, add_value (optional), and mul_value (optional) # We can pass these as Static Component variables since they are not needed elsewhere. comp1 . static_args = dict ( inp_image = inp_image , add_value = 10 , mul_value = 100 ) # We want to get the final value of `inp_image` since that will need to be passed to demo2. So, record it. comp1 . to_record = dict ( inp_image = 'inp_image' ) # Create the second Component comp2 = Component ( name = 'demo2' , func = 'demo2' ) # From the definition of demo1, we need to pass inp_image and threshold_value. The former is a # runtime argument that comes from the output of demo1, and threshold_value is a static argument. comp2 . static_args = dict ( threshold_value = 0.5 ) comp2 . runtime_args = dict ( inp_image = comp1 . future_variable . inp_image ) # i.e Get the value, at that time in the future when this function is executed, # stored by name `inp_image` in comp1 and pass it to comp2's function (i.e demo2) # as the kwarg value for `inp_image` # We do not want to record anything for this component since we don't need any output from it. # Create a pipeline and add these components pipeline = Pipeline ( name = 'demo' ) pipeline . compile_pipeline ([ comp1 , comp2 ]) # Run pipeline . run () Output RUNNING PIPELINE \"demo\" ... [ 2019 - 04 - 11 15 : 54 : 02 . 948031 ][ PID 21641 ][ n_time = 0 ] [ 2019 - 04 - 11 15 : 54 : 02 . 948138 ][ PID 21641 ][ Comp demo1 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 54 : 02 . 948152 ][ PID 21641 ][ Comp demo1 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 54 : 02 . 948393 ][ PID 21641 ][ Comp demo1 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 000151 [ 2019 - 04 - 11 15 : 54 : 02 . 948446 ][ PID 21641 ][ Comp demo2 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 54 : 02 . 948457 ][ PID 21641 ][ Comp demo2 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 54 : 02 . 948768 ][ PID 21641 ][ Comp demo2 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 000212","title":"Basic Pipeline"},{"location":"EXAMPLES/#printing-variables","text":"Code Example comp2 . to_print = [ 'dummy_variable' ] Output RUNNING PIPELINE \"demo\" ... [ 2019 - 04 - 11 15 : 56 : 42 . 365138 ][ PID 21956 ][ n_time = 0 ] [ 2019 - 04 - 11 15 : 56 : 42 . 365232 ][ PID 21956 ][ Comp demo1 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 56 : 42 . 365248 ][ PID 21956 ][ Comp demo1 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 56 : 42 . 365508 ][ PID 21956 ][ Comp demo1 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 000146 [ 2019 - 04 - 11 15 : 56 : 42 . 365544 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 56 : 42 . 365570 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 56 : 42 . 365774 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] to_print ... [ 2019 - 04 - 11 15 : 56 : 42 . 365792 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] [ to_print ] Value of dummy_variable [ 2019 - 04 - 11 15 : 56 : 42 . 365825 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] [ to_print ] 5 [ 2019 - 04 - 11 15 : 56 : 42 . 365960 ][ PID 21956 ][ Comp demo2 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 000316 # Note that you do not need to add a variable to `to_record` to print it.","title":"Printing Variables"},{"location":"EXAMPLES/#saving-variables","text":"Code Just like printing variables, you can also save them. To achieve this, use the to_save attribute of a Component. It accepts a dictionary, where a (key, value) can be of two types: - (key, value) = (str, str) : In this case, provide the key with the variable you are trying to save, and provide the full save path as the value . The method to save is inferred from the extension of the save path. Currently, saving numpy files ( .npy ), and images ( .png or .jpg ) is supported. - (key, value) = (str, function) : In this case, key remains the same as above, but in value provide a python function handle that would take in the positional argument (0th position) of the value of key at runtime and would save it according to the custom method. Example def dummy_save ( var ): print ( 'I am a dummy save method; will not save anything.' + 'But I confirm to have received the variable {}' . format ( var )) comp2 . to_save = dict ( inp_image = 'test_inp_image.npy' , dummy_variable = dummy_save ) Output RUNNING PIPELINE \"demo\" ... [ 2019 - 04 - 11 15 : 58 : 04 . 105548 ][ PID 22064 ][ n_time = 0 ] [ 2019 - 04 - 11 15 : 58 : 04 . 105655 ][ PID 22064 ][ Comp demo1 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 58 : 04 . 105669 ][ PID 22064 ][ Comp demo1 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 58 : 04 . 105897 ][ PID 22064 ][ Comp demo1 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 000140 [ 2019 - 04 - 11 15 : 58 : 04 . 105961 ][ PID 22064 ][ Comp demo2 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 15 : 58 : 04 . 105972 ][ PID 22064 ][ Comp demo2 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 15 : 58 : 04 . 106237 ][ PID 22064 ][ Comp demo2 ][ n_time = 0 ] Saving ... I am a dummy save method ; will not save anything . But I confirm to have received the variable 5 [ 2019 - 04 - 11 15 : 58 : 04 . 109076 ][ PID 22064 ][ Comp demo2 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 003014 # Also, test_inp_image.npy gets saved # Note that you do not need to add a variable to `to_record` to save it.","title":"Saving Variables"},{"location":"EXAMPLES/#adding-static-pipeline-variables","text":"Code In our basic example, let us say we use demo3 from the function library instead of demo2 (and as such, comp3 instead of comp2 ), and we want to use the same value for the argument add_value in both demo1 and demo3 . One way to do this is to add it as static_args in both the components. The other way is to add it as a static pipeline variable just once, and let both the components access it. Example inp_image = np . random . uniform ( 0 , 1 , size = ( 256 , 256 )) # Initialize a pipeline, and set the pipeline variable `add_value` pipeline = Pipeline ( name = 'demo' ) pipeline . add_pipeline_variable ( dict ( add_value = 5 )) comp1 = Component ( name = 'demo1' , func = 'demo1' ) comp1 . static_args = dict ( inp_image = inp_image , mul_value = 100 ) comp1 . runtime_args = dict ( add_value = pipeline . future_variable . add_value ) comp1 . to_record = dict ( inp_image = 'inp_image' ) comp3 = Component ( name = 'demo3' , func = 'demo3' ) comp3 . runtime_args = dict ( inp_image = comp1 . future_variable . inp_image , add_value = pipeline . future_variable . add_value ) # Note that `compile_pipeline` should be the last operation performed on the pipeline before # calling the `.run` on it pipeline . compile_pipeline ([ comp1 , comp3 ]) pipeline . run () Output # Outputs RUNNING PIPELINE \"demo\" ... [ 2019 - 04 - 11 16 : 06 : 19 . 808513 ][ PID 22861 ][ n_time = 0 ] [ 2019 - 04 - 11 16 : 06 : 19 . 808601 ][ PID 22861 ][ Comp demo1 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 16 : 06 : 19 . 808614 ][ PID 22861 ][ Comp demo1 ][ n_time = 0 ] Running Function ... [ 2019 - 04 - 11 16 : 06 : 19 . 808841 ][ PID 22861 ][ Comp demo1 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 00012 8 [ 2019 - 04 - 11 16 : 06 : 19 . 808874 ][ PID 22861 ][ Comp demo3 ][ n_time = 0 ] Compute Starting ... [ 2019 - 04 - 11 16 : 06 : 19 . 808885 ][ PID 22861 ][ Comp demo3 ][ n_time = 0 ] Running Function ... I received the add_value as : 5 [ 2019 - 04 - 11 16 : 06 : 19 . 809067 ][ PID 22861 ][ Comp demo3 ][ n_time = 0 ] Compute Ended . Took 0 : 00 : 00 . 0000 94","title":"Adding Static Pipeline Variables"},{"location":"EXAMPLES/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"EXAMPLES/#wrapping-pipeline-inside-another-pipeline","text":"Consider the case when you have a pipeline that you want to run inside a loop. A simple way to do this would be like Example for x in ... : pipeline . run ( kwargs = dict ( inp_x = x )) But if the for loop had to be part of a Component, which in turn was a part of a Pipeline, then there is a little more to do. Consider the example where you have a list of two dlkeys, and want to run a model over each of them. The inner most pipeline then looks like (get image) + (run model prediction). Example def get_image ( key , products ): dltile = dl . raster . dltile ( key ) img , _ = dl . raster . ndarray ( ... ) return locals () def model_predict ( model_path , inp_image ): model = load_model ( model_path ) output = model . predict ( inp_image ) return locals () comp0 = Component ( name = 'get_image' , func = get_image ) comp0 . static_args = dict ( products = [ 'usda:naip:rgbn:v1' ]) comp0 . to_record = dict ( img = 'out_image' ) # We do not provide any value for `key` right now since it will come from an outer pipeline at runtime. comp1 = Component ( name = 'model_predict' , func = model_predict ) comp1 . static_args = dict ( model_path = '...' ) comp1 . runtime_args = dict ( inp_image = comp0 . future_variable . out_image ) inner_pipeline = Pipeline ( name = 'inner_pipeline' ) inner_pipeline . compile_pipeline ([ comp0 , comp1 ]) # Now we need an outer pipeline that has one component that gets the keys and another component # that loops over the keys and passes them sequentially to the inner pipeline. # If required, you could append other components to the outer pipeline, and they will get executed once the # inner pipeline is done processing all the keys in the loop. def get_keys (): keys = [ '1024:0:1.0:-3:2' , '1024:0:1.0:-10:4' ] return locals () def loop_keys ( keys , pipeline ): for i in loop_keys : pipeline . run ( kwargs = dict ( key = i )) # This is where the argument `key` is getting passed to `get_image` of comp0. # Note that declaration of such a function loop_keys limits what pipeline can be used with it: # The first component of that pipeline \"should\" accept an argument for `key` return locals () comp_main0 = Component ( name = 'get_keys' , func = get_keys ) comp_main0 . to_record = dict ( keys = 'dlkeys' ) comp_main1 = Component ( name = 'loop_keys' , func = loop_keys ) comp_main1 . static_args = dict ( pipeline = inner_pipeline ) comp_main1 . runtime_args = dict ( keys = comp_main0 . future_variable . dlkeys ) outer_pipeline = Pipeline ( name = 'outer_pipeline' ) outer_pipeline . compile_pipeline ([ comp_main0 , comp_main1 ]) outer_pipeline . run ()","title":"Wrapping Pipeline inside another Pipeline"},{"location":"EXAMPLES/#sharing-component-variables-across-pipelines","text":"In the above example , you would notice that the same model is being loaded twice inside the inner pipeline. A better way to do this is to load the model once, in the outer pipeline, and then share the variable with the corresponding component in the inner pipeline. This applies to any such use case, where sharing removes redundancy. Example # Create a component for loading the model def model_loader ( model_path ): model = load_model ( model_path ) return locals () # Modify the definition of `model_predict` in the previous example to not load the model def model_predict ( model , inp_image ): output = model . predict ( inp_image ) return locals () # Create the outer pipeline comp_main0 = ... # Same as the get_keys component above comp_main1 = Component ( name = 'model_loader' , func = model_loader ) comp_main1 . static_args = dict ( model_path =... ) comp_main1 . to_record = dict ( model = 'loaded_model' ) comp_main2 = ... # Same as the loop_keys component above (called as comp_main1 in that example) outer_pipeline . compile_pipeline ([ comp_main0 , comp_main1 , comp_main2 ]) # Create the inner pipeline comp0 = ... # Same as in the above example comp1 = Component ( name = 'model_predict' , func = model_predict ) comp1 . runtime_args = dict ( inp_image = comp0 . future_variable . out_image , model = comp_main1 . future_variable . shared_model ) inner_pipeline . compile_pipeline ([ comp0 , comp1 ])","title":"Sharing Component Variables across Pipelines"},{"location":"EXAMPLES/#task-examples","text":"Some caution needs to be taken when defining pipelines that work with DLTasks. The main tricky point is that Tasks can only take a serializable function as its input. Therefore, any pipeline that you want to run within Tasks should be wrapped in a python function. Example: Wind Turbines Example","title":"Task Examples"},{"location":"EXAMPLES/#multiprocessing-examples","text":"See an example for multiprocessing within pipeline See an example for multiprocessing across pipeline","title":"Multiprocessing Examples"},{"location":"FUNC_DOCS/","text":"For documentation on functions and methods provided by the framework, look at READ THE DOCS Alternatively, you could do help ( Pipeline ) and help ( Component ) in the terminal.","title":"Functions and Methods"},{"location":"INSTALLATION/","text":"This framework does not use any libraries other than what the standard version of appsci_utils uses, with the exception that OpenCV is mandatory if you want to plot visual summaries of pipeline runs . Also make sure to use Python3.5+","title":"Installation"}]}